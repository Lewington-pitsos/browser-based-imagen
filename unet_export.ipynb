{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97423763",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "C:\\Python310\\lib\\site-packages\\beartype\\_util\\hint\\pep\\utilpeptest.py:345: BeartypeDecorHintPep585DeprecationWarning: PEP 484 type hint typing.List[str] deprecated by PEP 585 scheduled for removal in the first Python version released after October 5th, 2025. To resolve this, import this hint from \"beartype.typing\" rather than \"typing\". See this discussion for further details and alternatives:\n",
      "    https://github.com/beartype/beartype#pep-585-deprecations\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from imagen_pytorch import Unet, Imagen, ImagenTrainer\n",
    "from imagen_pytorch.data import Dataset\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torchvision.transforms as T\n",
    "from imagen_pytorch.t5 import t5_encode_text\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7b43094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading saved imagen at version 1.17.1, but current package version is 1.16.5\n",
      "checkpoint loaded from giddy-capybara.ckpt\n"
     ]
    }
   ],
   "source": [
    "unet = Unet(\n",
    "    dim = 128, # the \"Z\" layer dimension, i.e. the number of filters the outputs to the first layer\n",
    "    cond_dim = 128,\n",
    "    dim_mults = (1, 2, 4), # the channel dimensions inside the model (multiplied by dim)\n",
    "    num_resnet_blocks = 3,\n",
    "    layer_attns = (False, True, True),\n",
    "    layer_cross_attns = (False, True, True)\n",
    ")\n",
    "\n",
    "imagen = Imagen(\n",
    "    unets = unet,\n",
    "    image_sizes = 32,\n",
    "    timesteps = 250,\n",
    "    cond_drop_prob = 0.1,\n",
    "    dynamic_thresholding=False\n",
    ")\n",
    "\n",
    "\n",
    "trainer = ImagenTrainer(imagen)\n",
    "trainer.load(\"giddy-capybara.ckpt\")\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c520101c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4b8968cded847aba44bcc53b41e5a10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21411f37da92422d9275ac39175ba2a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1.0000, 0.9954, 0.9983,  ..., 0.9889, 0.9900, 0.9995],\n",
      "          [1.0000, 0.9890, 0.9967,  ..., 0.9946, 0.9922, 1.0000],\n",
      "          [0.9974, 0.9903, 0.9983,  ..., 1.0000, 0.9998, 0.9922],\n",
      "          ...,\n",
      "          [0.8649, 0.8513, 0.8566,  ..., 0.6962, 0.7132, 0.7229],\n",
      "          [0.8516, 0.8652, 0.8598,  ..., 0.7279, 0.7315, 0.7360],\n",
      "          [0.8598, 0.8672, 0.8869,  ..., 0.7618, 0.7636, 0.7461]],\n",
      "\n",
      "         [[1.0000, 1.0000, 0.9999,  ..., 1.0000, 0.9932, 1.0000],\n",
      "          [1.0000, 0.9965, 1.0000,  ..., 0.9927, 0.9934, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 0.9845, 0.9903, 0.9886],\n",
      "          ...,\n",
      "          [0.8574, 0.8630, 0.8554,  ..., 0.6927, 0.7096, 0.7203],\n",
      "          [0.8561, 0.8571, 0.8566,  ..., 0.7262, 0.7346, 0.7317],\n",
      "          [0.8546, 0.8594, 0.8644,  ..., 0.7539, 0.7529, 0.7291]],\n",
      "\n",
      "         [[1.0000, 1.0000, 0.9961,  ..., 0.9992, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 0.9957,  ..., 0.9993, 1.0000, 0.9990],\n",
      "          ...,\n",
      "          [0.8637, 0.8712, 0.8728,  ..., 0.7071, 0.7253, 0.7327],\n",
      "          [0.8538, 0.8583, 0.8751,  ..., 0.7418, 0.7540, 0.7528],\n",
      "          [0.8581, 0.8695, 0.8689,  ..., 0.7721, 0.7765, 0.7548]]]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=32x32>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = trainer.sample([\"a photo of a truck\"])\n",
    "print(out)\n",
    "img = T.ToPILImage()(out[0])\n",
    "img.save(\"truck-4.png\")\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "022bd40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = imagen.sample(text_embeds=enc, text_masks=mask)\n",
    "# print(out)\n",
    "# img = T.ToPILImage()(out[0])\n",
    "# img.save(\"truck-4.png\")\n",
    "# img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c737c97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagenOnnx(torch.nn.Module):\n",
    "    def __init__(self, unet, imagen):\n",
    "        super().__init__()\n",
    "        \n",
    "        if imagen.dynamic_thresholding == True or any(imagen.dynamic_thresholding):\n",
    "            raise ValueError(\"no dynamic thresholding allowed, got:\", imagen.dynamic_thresholding)\n",
    "\n",
    "        self.unet = unet\n",
    "        self.imagen = imagen    \n",
    "            \n",
    "\n",
    "    def sample(self, text_embeds, text_mask, cond_scale,  device, use_tqdm=True):\n",
    "        batch_size = text_embeds.shape[0]\n",
    "        noise_scheduler = self.imagen.noise_schedulers[0]\n",
    "        img = torch.randn((batch_size, self.imagen.sample_channels[0], self.imagen.image_sizes[0], self.imagen.image_sizes[0]), device = device)        \n",
    "        timesteps = noise_scheduler.get_sampling_timesteps(batch_size, device = device)\n",
    "            \n",
    "        \n",
    "        for times, times_next in tqdm(timesteps, desc='sampling loop time step', total=len(timesteps), disable=not use_tqdm):       \n",
    "            self_cond = x_start if self.unet.self_cond else None\n",
    "            \n",
    "            img, x_start = self.imagen.p_sample(\n",
    "                self.unet,\n",
    "                img,\n",
    "                times,\n",
    "                t_next = times_next,\n",
    "                text_embeds = text_embeds,\n",
    "                text_mask = text_mask,\n",
    "                cond_scale = cond_scale,\n",
    "                noise_scheduler = noise_scheduler,\n",
    "                pred_objective = self.imagen.pred_objectives[0],\n",
    "                dynamic_threshold = False\n",
    "            )\n",
    "\n",
    "        img.clamp_(-1., 1.)\n",
    "\n",
    "        unnormalize_img = self.imagen.unnormalize_img(img)\n",
    "\n",
    "        return unnormalize_img\n",
    "    \n",
    "    def forward(self, img, text_embeds, text_mask, times, times_next, cond_scale):\n",
    "        return self.imagen.p_sample(\n",
    "            self.unet,\n",
    "            img,\n",
    "            times,\n",
    "            t_next = times_next,\n",
    "            text_embeds = text_embeds,\n",
    "            text_mask = text_mask,\n",
    "            cond_scale = cond_scale,\n",
    "            noise_scheduler=imagen.noise_schedulers[0],\n",
    "            pred_objective = imagen.pred_objectives[0],\n",
    "            dynamic_threshold = False\n",
    "        )\n",
    "    \n",
    "u = ImagenOnnx(unet, imagen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "752abd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enc, mask = t5_encode_text([\"a photo of a truck\"], return_attn_mask = True)\n",
    "# out = u.sample(enc, mask, 1., torch.device('cuda'), use_tqdm=False)\n",
    "# print(out)\n",
    "# img = T.ToPILImage()(out[0])\n",
    "# img.save(\"truck-12.png\")\n",
    "# img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "94c57b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = u.forward(\n",
    "    torch.rand(1, 3, 32, 32).cuda(),  \n",
    "    torch.rand(1, 27, 768).cuda(), \n",
    "    torch.ones(1, 27, dtype=bool).cuda(), \n",
    "    torch.Tensor([0.9]).cuda(), \n",
    "    torch.Tensor([0.896]).cuda(), \n",
    "    torch.Tensor([1.]).cuda()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "88747347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[ 0.2748,  0.9250,  0.5351,  ...,  1.2733,  0.4265,  0.7702],\n",
       "           [ 0.8397,  0.7987,  0.7799,  ..., -0.2407,  0.6717,  0.9083],\n",
       "           [ 0.2817,  0.8886,  0.2003,  ...,  0.9885,  0.7546,  0.4452],\n",
       "           ...,\n",
       "           [-0.1932,  0.0788,  1.0260,  ...,  0.3314,  0.7054,  0.3481],\n",
       "           [ 0.3075,  0.0477,  0.5602,  ...,  0.9194,  0.0665,  0.8710],\n",
       "           [ 0.3930,  0.3563,  0.8796,  ...,  0.2235,  0.6368,  0.8849]],\n",
       " \n",
       "          [[ 1.1616,  0.6004,  0.7262,  ...,  0.5299,  0.0334,  0.4350],\n",
       "           [ 0.3489,  0.5846,  0.3296,  ...,  0.4249,  0.2732,  0.0524],\n",
       "           [-0.1354,  0.2545,  0.4192,  ...,  0.3111,  0.3653,  0.8367],\n",
       "           ...,\n",
       "           [ 0.8770,  0.5802,  0.6854,  ...,  0.5512,  0.7838,  0.0364],\n",
       "           [ 0.7926,  0.7147,  0.6851,  ..., -0.0146,  0.5998,  0.5217],\n",
       "           [ 0.3273,  0.0340,  0.2636,  ...,  0.2503,  0.8422,  1.1161]],\n",
       " \n",
       "          [[ 0.0133,  0.1382,  0.5957,  ...,  0.0732, -0.0968,  0.0388],\n",
       "           [ 0.4431, -0.0584,  0.1056,  ...,  0.8861,  0.1104,  0.1465],\n",
       "           [ 0.2401,  0.0602,  0.6047,  ...,  0.6328,  0.8370,  0.4505],\n",
       "           ...,\n",
       "           [ 0.7938,  0.6073,  0.7765,  ...,  0.4753,  0.5645,  0.6941],\n",
       "           [ 0.5149, -0.1084,  0.7724,  ..., -0.0447,  0.4829,  0.8395],\n",
       "           [ 0.4566,  1.0058,  0.5246,  ...,  1.1525, -0.0125,  0.5661]]]],\n",
       "        device='cuda:0'),\n",
       " tensor([[[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           ...,\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       " \n",
       "          [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           ...,\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       " \n",
       "          [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           ...,\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.]]]], device='cuda:0'))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "794e5a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = imagen.noise_schedulers[0].get_sampling_timesteps(1, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13b042cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ceb3312",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a61782e8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python310\\lib\\site-packages\\imagen_pytorch\\imagen_pytorch.py:2009: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert not (cond_scale != 1. and not self.can_classifier_guidance), 'imagen was not trained with conditional dropout, and thus one cannot use classifier free guidance (cond_scale anything other than 1)'\n",
      "C:\\Python310\\lib\\site-packages\\einops\\einops.py:204: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  inferred_length: int = length // known_product\n",
      "C:\\Python310\\lib\\site-packages\\imagen_pytorch\\imagen_pytorch.py:1583: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if remainder > 0:\n",
      "C:\\Python310\\lib\\site-packages\\imagen_pytorch\\imagen_pytorch.py:1587: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if remainder > 0:\n",
      "C:\\Python310\\lib\\site-packages\\torch\\nn\\functional.py:2515: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  _verify_batch_size([input.size(0) * input.size(1) // num_groups, num_groups] + list(input.size()[2:]))\n",
      "C:\\Python310\\lib\\site-packages\\einops\\packing.py:149: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  -1 if -1 in p_shape else prod(p_shape)\n",
      "C:\\Python310\\lib\\site-packages\\einops\\packing.py:154: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if n_unknown_composed_axes > 1:\n",
      "C:\\Python310\\lib\\site-packages\\einops\\packing.py:166: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if n_unknown_composed_axes == 0:\n",
      "C:\\Python310\\lib\\site-packages\\imagen_pytorch\\imagen_pytorch.py:1485: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if cond_scale == 1:\n"
     ]
    }
   ],
   "source": [
    "torch.onnx.export(\n",
    "    u, \n",
    "    (\n",
    "        torch.rand(1, 3, 32, 32).cuda(),  \n",
    "        torch.rand(1, 27, 768).cuda(), \n",
    "        torch.ones(1, 27, dtype=bool).cuda(), \n",
    "        torch.Tensor([0.5]).cuda(), \n",
    "        torch.Tensor([0.56]).cuda(), \n",
    "        torch.Tensor([1.1]).cuda() # cond scale\n",
    "    ), \n",
    "    \"toymodel/public/unet-32.onnx\", \n",
    "    input_names=['image', 'text_embeds', 'text_mask', 'timestep', 'time_next', 'cond_scale'], \n",
    "    output_names=['prediction', 'x_start'],\n",
    "    dynamic_axes={\n",
    "        'image': {0: 'batch_size'},\n",
    "        'text_embeds': {0: 'batch_size', 1: 'n_tokens'},\n",
    "        'text_mask': {0: 'batch_size', 1: 'n_tokens'},\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26c059c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
