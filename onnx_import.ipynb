{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17536cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "C:\\Python310\\lib\\site-packages\\beartype\\_util\\hint\\pep\\utilpeptest.py:345: BeartypeDecorHintPep585DeprecationWarning: PEP 484 type hint typing.List[str] deprecated by PEP 585 scheduled for removal in the first Python version released after October 5th, 2025. To resolve this, import this hint from \"beartype.typing\" rather than \"typing\". See this discussion for further details and alternatives:\n",
      "    https://github.com/beartype/beartype#pep-585-deprecations\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The base dimension of your u-net should ideally be no smaller than 128, as recommended by a professional DDPM trainer https://nonint.com/2022/05/04/friends-dont-let-friends-train-small-diffusion-models/\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "you did not supply the correct number of u-nets (1) for resolutions (64, 256)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 26\u001b[0m\n\u001b[0;32m      6\u001b[0m unet1 \u001b[38;5;241m=\u001b[39m Unet(\n\u001b[0;32m      7\u001b[0m     dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m,\n\u001b[0;32m      8\u001b[0m     cond_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m512\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m     layer_cross_attns \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     13\u001b[0m )\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# unet2 = Unet(\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m#     dim = 32,\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m#     cond_dim = 512,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m \n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# imagen, which contains the unets above (base unet and super resoluting ones)\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m imagen \u001b[38;5;241m=\u001b[39m \u001b[43mImagen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43munets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43munet1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_sizes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimesteps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcond_drop_prob\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\n\u001b[0;32m     31\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# mock images (get a lot of this) and text encodings from large T5\u001b[39;00m\n\u001b[0;32m     35\u001b[0m text_embeds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m768\u001b[39m)\u001b[38;5;241m.\u001b[39mcuda()\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\imagen_pytorch\\imagen_pytorch.py:1874\u001b[0m, in \u001b[0;36mImagen.__init__\u001b[1;34m(self, unets, image_sizes, text_encoder_name, text_embed_dim, channels, timesteps, cond_drop_prob, loss_type, noise_schedules, pred_objectives, random_crop_sizes, lowres_noise_schedule, lowres_sample_noise_level, per_sample_random_aug_noise_level, condition_on_text, auto_normalize_img, p2_loss_weight_gamma, p2_loss_weight_k, dynamic_thresholding, dynamic_thresholding_percentile, only_train_unet_number)\u001b[0m\n\u001b[0;32m   1871\u001b[0m image_sizes \u001b[38;5;241m=\u001b[39m cast_tuple(image_sizes)\n\u001b[0;32m   1872\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_sizes \u001b[38;5;241m=\u001b[39m image_sizes\n\u001b[1;32m-> 1874\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m num_unets \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(image_sizes), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myou did not supply the correct number of u-nets (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(unets)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) for resolutions \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_sizes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1876\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample_channels \u001b[38;5;241m=\u001b[39m cast_tuple(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchannels, num_unets)\n\u001b[0;32m   1878\u001b[0m \u001b[38;5;66;03m# determine whether we are training on images or video\u001b[39;00m\n",
      "\u001b[1;31mAssertionError\u001b[0m: you did not supply the correct number of u-nets (1) for resolutions (64, 256)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from imagen_pytorch import Unet, Imagen\n",
    "\n",
    "# unet for imagen\n",
    "\n",
    "unet1 = Unet(\n",
    "    dim = 32,\n",
    "    cond_dim = 512,\n",
    "    dim_mults = (1, 2, 4, 8),\n",
    "    num_resnet_blocks = 3,\n",
    "    layer_attns = (False, True, True, True),\n",
    "    layer_cross_attns = (False, True, True, True)\n",
    ")\n",
    "\n",
    "# unet2 = Unet(\n",
    "#     dim = 32,\n",
    "#     cond_dim = 512,\n",
    "#     dim_mults = (1, 2, 4, 8),\n",
    "#     num_resnet_blocks = (2, 4, 8, 8),\n",
    "#     layer_attns = (False, False, False, True),\n",
    "#     layer_cross_attns = (False, False, False, True)\n",
    "# )\n",
    "\n",
    "# imagen, which contains the unets above (base unet and super resoluting ones)\n",
    "\n",
    "imagen = Imagen(\n",
    "    unets = (unet1,),\n",
    "    image_sizes = (64,),\n",
    "    timesteps = 100,\n",
    "    cond_drop_prob = 0.1\n",
    ").cuda()\n",
    "\n",
    "# mock images (get a lot of this) and text encodings from large T5\n",
    "\n",
    "text_embeds = torch.randn(4, 256, 768).cuda()\n",
    "images = torch.randn(4, 3, 256, 256).cuda()\n",
    "\n",
    "# feed images into imagen, training each unet in the cascade\n",
    "\n",
    "for i in (1, 2):\n",
    "    loss = imagen(images, text_embeds = text_embeds, unet_number = i)\n",
    "    loss.backward()\n",
    "\n",
    "# do the above for many many many many steps\n",
    "# now you can sample an image based on the text embeddings from the cascading ddpm\n",
    "\n",
    "images = imagen.sample(texts = [\n",
    "    'a whale breaching from afar',\n",
    "#     'young girl blowing out candles on her birthday cake',\n",
    "#     'fireworks with blue and green sparkles'\n",
    "], cond_scale = 3.)\n",
    "\n",
    "images.shape # (3, 3, 256, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4b5008",
   "metadata": {},
   "outputs": [],
   "source": [
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b29c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "onnx_model = onnx.load('resnet18.onnx')\n",
    "\n",
    "graph = onnx_model.graph\n",
    "initalizers = dict()\n",
    "for init in graph.initializer:\n",
    "    initalizers[init.name] = numpy_helper.to_array(init)\n",
    "\n",
    "for name, p in model.named_parameters():\n",
    "    p.data = (torch.from_numpy(initalizers[name])).data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
